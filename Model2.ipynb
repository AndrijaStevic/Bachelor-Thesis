{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproductability\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the historical data\n",
    "data = pd.read_csv('historical_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the sentiment scores data\n",
    "sentiment_data = pd.read_csv('daily_sentiment_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter the required columns and sort by date and ticker\n",
    "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values(['Ticker', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Merge sentiment scores (neutral, positive, negative) with historical data\n",
    "sentiment_data['datetime'] = pd.to_datetime(sentiment_data['datetime'])\n",
    "data = pd.merge(data, sentiment_data[['datetime', 'neutral', 'positive', 'negative']], left_on='Date', right_on='datetime', how='left')\n",
    "\n",
    "# Drop extra datetime column\n",
    "data.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "# Fill NaN sentiment scores with 0 (assume neutral sentiment if missing)\n",
    "data[['neutral', 'positive', 'negative']] = data[['neutral', 'positive', 'negative']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Encode the Ticker column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "tickers_encoded = encoder.fit_transform(data[['Ticker']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add the encoded tickers as additional features\n",
    "encoded_columns = [f'Ticker_{i}' for i in range(tickers_encoded.shape[1])]\n",
    "encoded_df = pd.DataFrame(tickers_encoded, columns=encoded_columns, index=data.index)\n",
    "data = pd.concat([data.reset_index(drop=True), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Scale numerical features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "numerical_features = ['Open', 'High', 'Low', 'Volume']  # Exclude 'Close' from general scaling\n",
    "data[numerical_features] = scaler_features.fit_transform(data[numerical_features])\n",
    "\n",
    "# Separate scaler for 'Close' price\n",
    "scaler_close = MinMaxScaler(feature_range=(0, 1))\n",
    "data[['Close']] = scaler_close.fit_transform(data[['Close']])  # Scale only 'Close' separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentiment columns are already between 0 and 1, so we don't scale them again\n",
    "numerical_features_with_sentiment = numerical_features + ['neutral', 'positive', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Preprocessing function\n",
    "def preprocess_data(data, n_steps=10):\n",
    "    X, y = [], []\n",
    "    feature_columns = numerical_features_with_sentiment + ['Close'] + encoded_columns\n",
    "\n",
    "    for ticker in data['Ticker'].unique():\n",
    "        ticker_data = data[data['Ticker'] == ticker]  # Ensure sequences stay within the same stock\n",
    "        ticker_data = ticker_data[feature_columns].values\n",
    "\n",
    "        for i in range(n_steps, len(ticker_data)):\n",
    "            X.append(ticker_data[i - n_steps:i, :])  # Last n_steps rows as features\n",
    "            y.append(ticker_data[i, -len(encoded_columns)-1])  # Predict 'Close' price\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Preprocess the data\n",
    "X, y = preprocess_data(data, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 0.0012 - val_loss: 1.2282e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.3733e-04 - val_loss: 4.8738e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.4879e-04 - val_loss: 6.7030e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 1.6287e-04 - val_loss: 4.5538e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.6207e-04 - val_loss: 4.5724e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.2773e-04 - val_loss: 3.4069e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 9.4844e-05 - val_loss: 2.8831e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.3504e-04 - val_loss: 4.2721e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 1.0986e-04 - val_loss: 4.7709e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 9.8878e-05 - val_loss: 2.1437e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.0398e-04 - val_loss: 2.5892e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 9.5733e-05 - val_loss: 2.3771e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 9.6557e-05 - val_loss: 2.7223e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 7.4461e-05 - val_loss: 3.5729e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 8.5662e-05 - val_loss: 1.8258e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 8.7512e-05 - val_loss: 3.6819e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 8.2136e-05 - val_loss: 2.1142e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 5.9889e-05 - val_loss: 1.8487e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 5.3282e-05 - val_loss: 2.4648e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 8.4470e-05 - val_loss: 1.8149e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 6.8296e-05 - val_loss: 2.9150e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.9550e-05 - val_loss: 2.3573e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 6.5729e-05 - val_loss: 2.4321e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.5820e-05 - val_loss: 2.8933e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.7140e-05 - val_loss: 2.1001e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 5.0904e-05 - val_loss: 1.1168e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 5.1820e-05 - val_loss: 3.3687e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.6374e-05 - val_loss: 6.1286e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.2242e-05 - val_loss: 2.6553e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 5.7711e-05 - val_loss: 1.3151e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.9042e-05 - val_loss: 2.7656e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.3534e-05 - val_loss: 2.8601e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.9248e-05 - val_loss: 2.3145e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.7122e-05 - val_loss: 3.2872e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 5.0449e-05 - val_loss: 5.5688e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.1371e-05 - val_loss: 5.0510e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.8987e-05 - val_loss: 1.1547e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.7014e-05 - val_loss: 7.3831e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.3455e-05 - val_loss: 4.5672e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.5873e-05 - val_loss: 1.2698e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.5156e-05 - val_loss: 8.4252e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.3802e-05 - val_loss: 3.7991e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.3423e-05 - val_loss: 1.0143e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 3.3430e-05 - val_loss: 1.3815e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.9275e-05 - val_loss: 9.4585e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.9775e-05 - val_loss: 8.7711e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.5414e-05 - val_loss: 9.8778e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.2170e-05 - val_loss: 1.9807e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.8816e-05 - val_loss: 1.0150e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.3477e-05 - val_loss: 1.2662e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.5209e-05 - val_loss: 9.7068e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.6434e-05 - val_loss: 1.0891e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.2790e-05 - val_loss: 8.7463e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.6840e-05 - val_loss: 8.8526e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.3750e-05 - val_loss: 2.1432e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.5822e-05 - val_loss: 1.0797e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.6304e-05 - val_loss: 1.5225e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.6264e-05 - val_loss: 1.0029e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.6931e-05 - val_loss: 1.6895e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.7943e-05 - val_loss: 1.2404e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.1543e-05 - val_loss: 1.1074e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.3295e-05 - val_loss: 1.3521e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.3521e-05 - val_loss: 1.4320e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.7367e-05 - val_loss: 1.1213e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.3362e-05 - val_loss: 1.0295e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.2595e-05 - val_loss: 1.7142e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.9094e-05 - val_loss: 1.2348e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.2117e-05 - val_loss: 1.3813e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2624e-05 - val_loss: 1.3920e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.5750e-05 - val_loss: 9.0846e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.6211e-05 - val_loss: 1.3715e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.8409e-05 - val_loss: 1.6948e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.9335e-05 - val_loss: 3.0376e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.5615e-05 - val_loss: 1.3979e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0952e-05 - val_loss: 2.0423e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0541e-05 - val_loss: 1.3334e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.1759e-05 - val_loss: 1.0761e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.8575e-05 - val_loss: 1.0500e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.6802e-05 - val_loss: 1.7254e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.4147e-05 - val_loss: 9.5970e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2183e-05 - val_loss: 2.1335e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.5960e-05 - val_loss: 1.2494e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.9652e-05 - val_loss: 1.0332e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.3741e-05 - val_loss: 1.4765e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.9213e-05 - val_loss: 1.3489e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.6695e-05 - val_loss: 1.5780e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.3562e-05 - val_loss: 1.5833e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.2941e-05 - val_loss: 1.1968e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.9660e-05 - val_loss: 1.4083e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.8799e-05 - val_loss: 2.0577e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.9100e-05 - val_loss: 1.3184e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.0830e-05 - val_loss: 1.3766e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.6728e-05 - val_loss: 2.0020e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0385e-05 - val_loss: 1.1935e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2540e-05 - val_loss: 1.3184e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.7471e-05 - val_loss: 1.4526e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.2008e-05 - val_loss: 1.6868e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.1365e-05 - val_loss: 1.3772e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.8916e-05 - val_loss: 1.4378e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.7810e-05 - val_loss: 1.5287e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Inverse transform only the Close price using the trained scaler\n",
    "y_test_scaled = scaler_close.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "predictions_scaled = scaler_close.inverse_transform(predictions.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: Evaluate the model\n",
    "mae = mean_absolute_error(predictions_scaled, y_test_scaled)\n",
    "mape = mean_absolute_percentage_error(predictions_scaled, y_test_scaled)\n",
    "\n",
    "# Define a percentage threshold for accuracy\n",
    "threshold_percentage = 5  # 5% tolerance\n",
    "\n",
    "# Calculate percentage errors\n",
    "percentage_errors = np.abs((y_test_scaled - predictions_scaled) / y_test_scaled) * 100\n",
    "\n",
    "# Count predictions within the threshold\n",
    "acc = np.mean(percentage_errors <= threshold_percentage) * 100\n",
    "\n",
    "# Alternative accuracy metric\n",
    "acc2 = (1 - mape) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error = 8.74949377771233\n",
      "Mean Absolute Percentage Error = 4.40%\n",
      "Accuracy with threshold = 68.83%\n",
      "Accuracy = 95.60%\n"
     ]
    }
   ],
   "source": [
    "# Step 17: Print performance metrics\n",
    "print(f\"Mean Absolute Error = {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error = {mape*100:.2f}%\")\n",
    "print(f\"Accuracy with threshold = {acc:.2f}%\")\n",
    "print(f\"Accuracy = {acc2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
