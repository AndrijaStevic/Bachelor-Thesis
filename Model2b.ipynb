{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproductability\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the historical data\n",
    "data = pd.read_csv('historical_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the sentiment scores data\n",
    "sentiment_data = pd.read_csv('daily_sentiment_scores_both.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter the required columns and sort by date and ticker\n",
    "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values(['Ticker', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Merge sentiment scores (neutral, positive, negative) with historical data\n",
    "sentiment_data['datetime'] = pd.to_datetime(sentiment_data['datetime'])\n",
    "data = pd.merge(data, sentiment_data[['datetime', 'neutral', 'positive', 'negative']], left_on='Date', right_on='datetime', how='left')\n",
    "\n",
    "# Drop extra datetime column\n",
    "data.drop(columns=['datetime'], inplace=True)\n",
    "\n",
    "# Fill NaN sentiment scores with 0 (assume neutral sentiment if missing)\n",
    "data[['neutral', 'positive', 'negative']] = data[['neutral', 'positive', 'negative']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Encode the Ticker column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "tickers_encoded = encoder.fit_transform(data[['Ticker']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add the encoded tickers as additional features\n",
    "encoded_columns = [f'Ticker_{i}' for i in range(tickers_encoded.shape[1])]\n",
    "encoded_df = pd.DataFrame(tickers_encoded, columns=encoded_columns, index=data.index)\n",
    "data = pd.concat([data.reset_index(drop=True), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Scale numerical features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "numerical_features = ['Open', 'High', 'Low', 'Volume']  # Exclude 'Close' from general scaling\n",
    "data[numerical_features] = scaler_features.fit_transform(data[numerical_features])\n",
    "\n",
    "# Separate scaler for 'Close' price\n",
    "scaler_close = MinMaxScaler(feature_range=(0, 1))\n",
    "data[['Close']] = scaler_close.fit_transform(data[['Close']])  # Scale only 'Close' separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentiment columns are already between 0 and 1, so we don't scale them again\n",
    "numerical_features_with_sentiment = numerical_features + ['neutral', 'positive', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Preprocessing function\n",
    "def preprocess_data(data, n_steps=10):\n",
    "    X, y = [], []\n",
    "    feature_columns = numerical_features_with_sentiment + ['Close'] + encoded_columns\n",
    "\n",
    "    for ticker in data['Ticker'].unique():\n",
    "        ticker_data = data[data['Ticker'] == ticker]  # Ensure sequences stay within the same stock\n",
    "        ticker_data = ticker_data[feature_columns].values\n",
    "\n",
    "        for i in range(n_steps, len(ticker_data)):\n",
    "            X.append(ticker_data[i - n_steps:i, :])  # Last n_steps rows as features\n",
    "            y.append(ticker_data[i, -len(encoded_columns)-1])  # Predict 'Close' price\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Preprocess the data\n",
    "X, y = preprocess_data(data, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.0012 - val_loss: 1.2071e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 2.3534e-04 - val_loss: 5.0713e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.6201e-04 - val_loss: 5.0070e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.5140e-04 - val_loss: 2.8724e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.6696e-04 - val_loss: 6.4884e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.3114e-04 - val_loss: 4.3708e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 9.9010e-05 - val_loss: 3.6807e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.3298e-04 - val_loss: 3.9527e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.1312e-04 - val_loss: 3.6569e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 9.7790e-05 - val_loss: 1.2951e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.0421e-04 - val_loss: 3.0451e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 9.4003e-05 - val_loss: 1.7824e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 8.7885e-05 - val_loss: 3.4241e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 7.4431e-05 - val_loss: 4.0127e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 8.6333e-05 - val_loss: 1.9714e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 8.4365e-05 - val_loss: 4.9721e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 7.1042e-05 - val_loss: 1.7482e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 5.6560e-05 - val_loss: 2.4523e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.2045e-05 - val_loss: 3.4288e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 7.9816e-05 - val_loss: 2.1101e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 7.0247e-05 - val_loss: 4.4517e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 5.5817e-05 - val_loss: 4.3931e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 6.3048e-05 - val_loss: 1.6941e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.1790e-05 - val_loss: 4.2298e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.9838e-05 - val_loss: 4.3531e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.6901e-05 - val_loss: 1.1791e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.8137e-05 - val_loss: 3.5005e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 5.2272e-05 - val_loss: 7.9899e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 4.8765e-05 - val_loss: 3.2590e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.9986e-05 - val_loss: 1.6932e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 4.7722e-05 - val_loss: 3.6042e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.8580e-05 - val_loss: 4.0984e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.9039e-05 - val_loss: 2.5840e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.4746e-05 - val_loss: 6.4284e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.9785e-05 - val_loss: 7.7549e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.7912e-05 - val_loss: 8.0149e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.8121e-05 - val_loss: 1.5149e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.6173e-05 - val_loss: 8.7329e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.1980e-05 - val_loss: 5.7691e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.2521e-05 - val_loss: 1.7764e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 3.2864e-05 - val_loss: 1.0951e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.3219e-05 - val_loss: 6.6613e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.1033e-05 - val_loss: 1.3031e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.0719e-05 - val_loss: 1.6559e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.8987e-05 - val_loss: 1.4139e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 3.0305e-05 - val_loss: 1.0700e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.3443e-05 - val_loss: 1.0709e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 3.0497e-05 - val_loss: 2.3146e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.9574e-05 - val_loss: 1.3828e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 3.5681e-05 - val_loss: 1.4541e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.5933e-05 - val_loss: 1.2217e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.5385e-05 - val_loss: 1.2962e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.0991e-05 - val_loss: 1.1620e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.3990e-05 - val_loss: 1.0495e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.4479e-05 - val_loss: 2.2264e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 2.5804e-05 - val_loss: 1.3298e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.7388e-05 - val_loss: 1.7517e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.4376e-05 - val_loss: 1.2492e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.8430e-05 - val_loss: 1.9599e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.9651e-05 - val_loss: 1.5705e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.1609e-05 - val_loss: 1.3465e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.0845e-05 - val_loss: 1.6700e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.5909e-05 - val_loss: 1.4022e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2.8636e-05 - val_loss: 1.1928e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.4085e-05 - val_loss: 1.1271e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.3516e-05 - val_loss: 1.9094e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.9558e-05 - val_loss: 1.3503e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.4509e-05 - val_loss: 1.6308e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.3244e-05 - val_loss: 1.5900e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 3.3234e-05 - val_loss: 1.2120e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.8741e-05 - val_loss: 1.4723e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.8946e-05 - val_loss: 1.7913e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.1458e-05 - val_loss: 3.0669e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.4912e-05 - val_loss: 1.5478e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.1697e-05 - val_loss: 2.1734e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.1224e-05 - val_loss: 1.6511e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 2.0548e-05 - val_loss: 1.1905e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.0030e-05 - val_loss: 1.1885e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 2.6701e-05 - val_loss: 1.9446e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.4271e-05 - val_loss: 1.1553e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 2.2193e-05 - val_loss: 2.3409e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.6745e-05 - val_loss: 1.5591e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.0619e-05 - val_loss: 1.4882e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.4031e-05 - val_loss: 1.5745e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0041e-05 - val_loss: 1.6970e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 2.7619e-05 - val_loss: 1.6771e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.5188e-05 - val_loss: 1.7659e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.4239e-05 - val_loss: 1.5124e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.2080e-05 - val_loss: 1.7489e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.9606e-05 - val_loss: 2.2849e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.9775e-05 - val_loss: 1.6699e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.1426e-05 - val_loss: 1.7484e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.6715e-05 - val_loss: 2.3208e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2220e-05 - val_loss: 1.4744e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.3191e-05 - val_loss: 1.6595e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.7585e-05 - val_loss: 1.8725e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2289e-05 - val_loss: 2.0704e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 2.3458e-05 - val_loss: 1.5538e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.8522e-05 - val_loss: 1.9090e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 2.1028e-05 - val_loss: 1.8557e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Inverse transform only the Close price using the trained scaler\n",
    "y_test_scaled = scaler_close.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "predictions_scaled = scaler_close.inverse_transform(predictions.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: Evaluate the model\n",
    "mae = mean_absolute_error(predictions_scaled, y_test_scaled)\n",
    "mape = mean_absolute_percentage_error(predictions_scaled, y_test_scaled)\n",
    "\n",
    "# Define a percentage threshold for accuracy\n",
    "threshold_percentage = 5  # 5% tolerance\n",
    "\n",
    "# Calculate percentage errors\n",
    "percentage_errors = np.abs((y_test_scaled - predictions_scaled) / y_test_scaled) * 100\n",
    "\n",
    "# Count predictions within the threshold\n",
    "acc = np.mean(percentage_errors <= threshold_percentage) * 100\n",
    "\n",
    "# Alternative accuracy metric\n",
    "acc2 = (1 - mape) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error = 17.77577018142301\n",
      "Mean Absolute Percentage Error = 15.03%\n",
      "Accuracy with threshold = 34.44%\n",
      "Accuracy = 84.97%\n"
     ]
    }
   ],
   "source": [
    "# Step 17: Print performance metrics\n",
    "print(f\"Mean Absolute Error = {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error = {mape*100:.2f}%\")\n",
    "print(f\"Accuracy with threshold = {acc:.2f}%\")\n",
    "print(f\"Accuracy = {acc2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
