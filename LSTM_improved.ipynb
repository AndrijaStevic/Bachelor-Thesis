{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the historical data\n",
    "data = pd.read_csv('historical_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter the required columns and sort by date and ticker\n",
    "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values(['Ticker', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Encode the Ticker column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "tickers_encoded = encoder.fit_transform(data[['Ticker']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add the encoded tickers as additional features\n",
    "encoded_columns = [f'Ticker_{i}' for i in range(tickers_encoded.shape[1])]\n",
    "encoded_df = pd.DataFrame(tickers_encoded, columns=encoded_columns, index=data.index)\n",
    "data = pd.concat([data.reset_index(drop=True), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Scale numerical features (Open, High, Low, Close, Volume)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Preprocessing function\n",
    "def preprocess_data(data, n_steps=10):\n",
    "    X, y = [], []\n",
    "    feature_columns = numerical_features + encoded_columns\n",
    "    for ticker in data['Ticker'].unique():\n",
    "        ticker_data = data[data['Ticker'] == ticker] # Ensuring sequences are within the same stock\n",
    "        ticker_data = ticker_data[feature_columns].values\n",
    "        for i in range(n_steps, len(ticker_data)):\n",
    "            X.append(ticker_data[i - n_steps:i, :])  # Last n_steps rows as features\n",
    "            y.append(ticker_data[i, 3])  # Predict 'Close' price\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Preprocess the data\n",
    "X, y = preprocess_data(data, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30105, 10, 103)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation=\"tanh\", return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.15),\n",
    "    LSTM(30, activation=\"tanh\", return_sequences=True),\n",
    "    Dropout(0.05),\n",
    "    LSTM(20, activation=\"tanh\", return_sequences=False),\n",
    "    Dropout(0.01),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - loss: 9.1102e-04 - val_loss: 5.2892e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.1879e-04 - val_loss: 3.1937e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 7.8338e-05 - val_loss: 6.9376e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 8.1107e-05 - val_loss: 6.8381e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 9.5806e-05 - val_loss: 3.1687e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 7.4018e-05 - val_loss: 2.4504e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 7.0062e-05 - val_loss: 3.0914e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 6.4513e-05 - val_loss: 6.0543e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 6.0726e-05 - val_loss: 3.0102e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 5.2503e-05 - val_loss: 2.5798e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 4.9150e-05 - val_loss: 2.4284e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.9544e-05 - val_loss: 2.2387e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 5.0791e-05 - val_loss: 1.7573e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.2607e-05 - val_loss: 1.2958e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.6632e-05 - val_loss: 1.5964e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.6505e-05 - val_loss: 3.5514e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.7900e-05 - val_loss: 3.3888e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.3333e-05 - val_loss: 1.4552e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.9900e-05 - val_loss: 5.1576e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 5.0276e-05 - val_loss: 1.8406e-05\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Inverse scale the predictions and actual values\n",
    "y_test_scaled = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((y_test.shape[0], 4)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
    "predictions_scaled = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((predictions.shape[0], 4)), predictions), axis=1))[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Evaluate the model\n",
    "mae = mean_absolute_error(predictions_scaled, y_test_scaled)\n",
    "mape = mean_absolute_percentage_error(predictions_scaled, y_test_scaled)\n",
    "\n",
    "# Define a percentage threshold for accuracy\n",
    "threshold_percentage = 5  # 5% tolerance\n",
    "\n",
    "# Calculate percentage errors\n",
    "percentage_errors = np.abs((y_test_scaled - predictions_scaled) / y_test_scaled) * 100\n",
    "\n",
    "# Count predictions within the threshold\n",
    "acc = np.mean(percentage_errors <= threshold_percentage) * 100\n",
    "\n",
    "acc2 = 1- mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error = 3876087.8863592604\n",
      "Mean Absolute Percentage Error = 7.31%\n",
      "Accuracy = 55.61%\n",
      "Accuracy = 92.69%\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Print performance metrics\n",
    "print(f\"Mean Absolute Error = {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error = {mape*100:.2f}%\")\n",
    "print(f\"Accuracy = {acc:.2f}%\")\n",
    "print(f\"Accuracy = {acc2 *100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
