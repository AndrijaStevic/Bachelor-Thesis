{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproductability\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the historical data\n",
    "data = pd.read_csv('historical_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter the required columns and sort by date and ticker\n",
    "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker']]\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values(['Ticker', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Encode the Ticker column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "tickers_encoded = encoder.fit_transform(data[['Ticker']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add the encoded tickers as additional features\n",
    "encoded_columns = [f'Ticker_{i}' for i in range(tickers_encoded.shape[1])]\n",
    "encoded_df = pd.DataFrame(tickers_encoded, columns=encoded_columns, index=data.index)\n",
    "data = pd.concat([data.reset_index(drop=True), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Scale numerical features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "numerical_features = ['Open', 'High', 'Low', 'Volume']  # Exclude 'Close' from general scaling\n",
    "data[numerical_features] = scaler_features.fit_transform(data[numerical_features])\n",
    "\n",
    "# Separate scaler for 'Close' price\n",
    "scaler_close = MinMaxScaler(feature_range=(0, 1))\n",
    "data[['Close']] = scaler_close.fit_transform(data[['Close']])  # Scale only 'Close' separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Preprocessing function\n",
    "def preprocess_data(data, n_steps=10):\n",
    "    X, y = [], []\n",
    "    feature_columns = numerical_features + ['Close'] + encoded_columns  # Include 'Close' now\n",
    "\n",
    "    for ticker in data['Ticker'].unique():\n",
    "        ticker_data = data[data['Ticker'] == ticker]  # Ensure sequences stay within the same stock\n",
    "        ticker_data = ticker_data[feature_columns].values\n",
    "\n",
    "        for i in range(n_steps, len(ticker_data)):\n",
    "            X.append(ticker_data[i - n_steps:i, :])  # Last n_steps rows as features\n",
    "            y.append(ticker_data[i, -len(encoded_columns)-1])  # Predict 'Close' price\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Preprocess the data\n",
    "X, y = preprocess_data(data, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation=\"tanh\", return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.15),\n",
    "    LSTM(30, activation=\"tanh\", return_sequences=True),\n",
    "    Dropout(0.05),\n",
    "    LSTM(20, activation=\"tanh\", return_sequences=False),\n",
    "    Dropout(0.01),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - loss: 9.3873e-04 - val_loss: 1.7506e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.2243e-04 - val_loss: 9.7737e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 1.6359e-04 - val_loss: 3.4518e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 1.0430e-04 - val_loss: 6.5942e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 9.1930e-05 - val_loss: 2.9606e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 8.9725e-05 - val_loss: 3.2006e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 7.6743e-05 - val_loss: 2.7421e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 8.6229e-05 - val_loss: 2.2883e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 6.7629e-05 - val_loss: 4.8178e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.0620e-05 - val_loss: 2.1867e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 7.1844e-05 - val_loss: 2.1765e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 5.3946e-05 - val_loss: 2.3195e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 7.6915e-05 - val_loss: 1.4097e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 4.4328e-05 - val_loss: 4.4713e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 6.1307e-05 - val_loss: 3.2332e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.2187e-05 - val_loss: 2.7262e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 6.4739e-05 - val_loss: 1.9031e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.6616e-05 - val_loss: 2.9079e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.6429e-05 - val_loss: 1.0795e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.4384e-05 - val_loss: 2.9085e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 5.5305e-05 - val_loss: 1.5796e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.1759e-05 - val_loss: 1.8495e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.7072e-05 - val_loss: 1.5379e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.8224e-05 - val_loss: 1.8750e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 4.1880e-05 - val_loss: 9.1751e-06\n",
      "Epoch 26/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.3827e-05 - val_loss: 1.5798e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.6158e-05 - val_loss: 1.9294e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 4.1752e-05 - val_loss: 8.0340e-06\n",
      "Epoch 29/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 3.1985e-05 - val_loss: 2.2108e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.6624e-05 - val_loss: 3.0043e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.7373e-05 - val_loss: 7.6765e-06\n",
      "Epoch 32/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.7863e-05 - val_loss: 1.0155e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.9717e-05 - val_loss: 1.2090e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.2267e-05 - val_loss: 3.0091e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 3.0241e-05 - val_loss: 1.3773e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.6273e-05 - val_loss: 1.4784e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.2598e-05 - val_loss: 2.5122e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.3544e-05 - val_loss: 7.9652e-06\n",
      "Epoch 39/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 3.6314e-05 - val_loss: 1.7424e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 3.3719e-05 - val_loss: 2.5129e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 4.1570e-05 - val_loss: 2.8202e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 3.6789e-05 - val_loss: 1.6380e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.3415e-05 - val_loss: 6.8203e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.5128e-05 - val_loss: 1.0226e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.3055e-05 - val_loss: 1.2724e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.0606e-05 - val_loss: 2.2363e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 3.0817e-05 - val_loss: 5.9131e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.4289e-05 - val_loss: 6.9660e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.0216e-05 - val_loss: 1.3067e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 3.0574e-05 - val_loss: 1.5741e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 3.0468e-05 - val_loss: 1.2226e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.7550e-05 - val_loss: 8.6394e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.5100e-05 - val_loss: 7.4558e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.7667e-05 - val_loss: 1.9885e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.5146e-05 - val_loss: 1.0578e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.5255e-05 - val_loss: 6.4727e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 3.0425e-05 - val_loss: 8.0249e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2.0241e-05 - val_loss: 9.7554e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 3.0721e-05 - val_loss: 1.0633e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 2.3316e-05 - val_loss: 1.0864e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 2.8147e-05 - val_loss: 1.7189e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - loss: 2.8106e-05 - val_loss: 8.8015e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0062e-05 - val_loss: 8.5134e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.6352e-05 - val_loss: 9.6799e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.4519e-05 - val_loss: 1.0960e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.4953e-05 - val_loss: 1.4924e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2.3624e-05 - val_loss: 5.1476e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - loss: 2.2588e-05 - val_loss: 6.9926e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - loss: 2.8326e-05 - val_loss: 5.0682e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2.4940e-05 - val_loss: 8.5276e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 1.9605e-05 - val_loss: 1.4638e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.4110e-05 - val_loss: 7.5092e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0251e-05 - val_loss: 8.5945e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.5770e-05 - val_loss: 8.5660e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.8710e-05 - val_loss: 8.2064e-06\n",
      "Epoch 76/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2.4289e-05 - val_loss: 7.0831e-06\n",
      "Epoch 77/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - loss: 2.0568e-05 - val_loss: 9.8240e-06\n",
      "Epoch 78/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.0843e-05 - val_loss: 4.7958e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.2531e-05 - val_loss: 1.0783e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.9128e-05 - val_loss: 6.8602e-06\n",
      "Epoch 81/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.1958e-05 - val_loss: 1.2481e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.3824e-05 - val_loss: 1.0330e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.9044e-05 - val_loss: 1.8677e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 2.6060e-05 - val_loss: 4.4977e-06\n",
      "Epoch 85/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.0513e-05 - val_loss: 9.3915e-06\n",
      "Epoch 86/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.0088e-05 - val_loss: 1.3108e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.1343e-05 - val_loss: 5.4884e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2140e-05 - val_loss: 7.6376e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 1.9712e-05 - val_loss: 7.6175e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.0687e-05 - val_loss: 1.0653e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.8078e-05 - val_loss: 1.0077e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.0800e-05 - val_loss: 6.5794e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.3259e-05 - val_loss: 5.5643e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.8274e-05 - val_loss: 8.0461e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 2.2339e-05 - val_loss: 8.7161e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.7946e-05 - val_loss: 5.3615e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 1.9816e-05 - val_loss: 5.6817e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.0684e-05 - val_loss: 8.3452e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 2.0204e-05 - val_loss: 1.3028e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 1.8052e-05 - val_loss: 1.9763e-05\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Inverse scale the predictions and actual values correctly\n",
    "y_test_scaled = scaler_close.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "predictions_scaled = scaler_close.inverse_transform(predictions.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Evaluate the model\n",
    "mae = mean_absolute_error(predictions_scaled, y_test_scaled)\n",
    "mape = mean_absolute_percentage_error(predictions_scaled, y_test_scaled)\n",
    "\n",
    "# Define a percentage threshold for accuracy\n",
    "threshold_percentage = 5  # 5% tolerance\n",
    "percentage_errors = np.abs((y_test_scaled - predictions_scaled) / y_test_scaled) * 100\n",
    "acc = np.mean(percentage_errors <= threshold_percentage) * 100\n",
    "\n",
    "# Alternative accuracy metric\n",
    "acc2 = (1 - mape) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error = 8.528863920876589\n",
      "Mean Absolute Percentage Error = 14.55%\n",
      "Accuracy with threshold = 44.97%\n",
      "Accuracy = 85.45%\n"
     ]
    }
   ],
   "source": [
    "# Step 15: Print performance metrics\n",
    "print(f\"Mean Absolute Error = {mae}\")\n",
    "print(f\"Mean Absolute Percentage Error = {mape*100:.2f}%\")\n",
    "print(f\"Accuracy with threshold = {acc:.2f}%\")\n",
    "print(f\"Accuracy = {acc2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
