{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load financial news dataset\n",
    "news_data = pd.read_csv('financialNews.csv')  # Replace with your actual dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tickers\n",
    "tickers = news_data['ticker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get historical data for all tickers\n",
    "def get_historical_data(tickers, start=\"2022-01-01\", end=\"2023-12-31\"):\n",
    "    historical_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start, end=end)\n",
    "            data['Ticker'] = ticker\n",
    "            historical_data[ticker] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    return historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Fetch Yahoo Finance historical data\n",
    "historical_data = get_historical_data(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ticker data into a single DataFrame\n",
    "historical_df = pd.concat(historical_data.values())\n",
    "historical_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for LSTM\n",
    "def preprocess_data(data, target_col):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data[[target_col]].values)\n",
    "    x, y = [], []\n",
    "    for i in range(60, len(scaled_data)):\n",
    "        x.append(scaled_data[i-60:i, 0])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    return np.array(x), np.array(y), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\astev\\AppData\\Local\\Temp\\ipykernel_24108\\3013397276.py\", line 11, in <module>\n",
      "    news_data = calculate_sentiment_scores(news_data)\n",
      "  File \"C:\\Users\\astev\\AppData\\Local\\Temp\\ipykernel_24108\\3013397276.py\", line 6, in calculate_sentiment_scores\n",
      "    news_df['sentiment'] = news_df['description'].apply(lambda x: finbert(x[:max_len])[0]['label'])\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py\", line 4764, in apply\n",
      "    ).apply()\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py\", line 1209, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py\", line 1289, in apply_standard\n",
      "    mapped = obj._map_values(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py\", line 921, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py\", line 1814, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "  File \"lib.pyx\", line 2926, in pandas._libs.lib.map_infer\n",
      "  File \"C:\\Users\\astev\\AppData\\Local\\Temp\\ipykernel_24108\\3013397276.py\", line 6, in <lambda>\n",
      "    news_df['sentiment'] = news_df['description'].apply(lambda x: finbert(x[:max_len])[0]['label'])\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\", line 159, in __call__\n",
      "    result = super().__call__(*inputs, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1302, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1309, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1204, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\", line 190, in _forward\n",
      "    return self.model(**model_inputs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 437, in run_call_with_unpacked_inputs\n",
      "    return func(self, **unpacked_inputs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1746, in call\n",
      "    outputs = self.bert(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 437, in run_call_with_unpacked_inputs\n",
      "    return func(self, **unpacked_inputs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 969, in call\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 609, in call\n",
      "    layer_outputs = layer_module(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n",
      "    intermediate_output = self.intermediate(hidden_states=attention_output)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "    outputs = call_fn(inputs, *args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 432, in call\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\activations.py\", line 348, in gelu\n",
      "    return tf.nn.gelu(x, approximate)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 3753, in gelu\n",
      "    features / math_ops.cast(1.4142135623730951, features.dtype)))\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1478, in binary_op_wrapper\n",
      "    return func(x, y, name=name)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1661, in truediv\n",
      "    return _truediv_python3(x, y, name)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 142, in wrapper\n",
      "    return op(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1599, in _truediv_python3\n",
      "    return gen_math_ops.real_div(x, y, name=name)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\", line 142, in wrapper\n",
      "    return op(*args, **kwargs)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 8089, in real_div\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py\", line 1129, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"c:\\Users\\astev\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py\", line 950, in findsource\n",
      "    raise OSError('source code not available')\n",
      "OSError: source code not available\n"
     ]
    }
   ],
   "source": [
    "# Apply FinBERT for sentiment analysis\n",
    "finbert = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def calculate_sentiment_scores(news_df):\n",
    "    max_len = 512  # FinBERT max token limit\n",
    "    news_df['sentiment'] = news_df['description'].apply(lambda x: finbert(x[:max_len])[0]['label'])\n",
    "    news_df['sentiment_score'] = news_df['sentiment'].map({\"POSITIVE\": 1, \"NEGATIVE\": -1, \"NEUTRAL\": 0})\n",
    "    return news_df\n",
    "\n",
    "\n",
    "news_data = calculate_sentiment_scores(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       datetime  \\\n",
      "0      Fri 24 Nov 2023, 12:00AM   \n",
      "1      Thu 23 Nov 2023, 07:00PM   \n",
      "2      Thu 23 Nov 2023, 05:43PM   \n",
      "3      Thu 23 Nov 2023, 04:47PM   \n",
      "4      Thu 23 Nov 2023, 03:25PM   \n",
      "...                         ...   \n",
      "95454  Mon 02 May 2022, 04:56AM   \n",
      "95455  Mon 02 May 2022, 04:56AM   \n",
      "95456  Mon 02 May 2022, 04:56AM   \n",
      "95457  Mon 02 May 2022, 04:56AM   \n",
      "95458  Mon 02 May 2022, 04:00AM   \n",
      "\n",
      "                                                   title  \\\n",
      "0      OpenAI turmoil exposes threat to Microsoft’s i...   \n",
      "1      10 Can’t Miss Black Friday Electronics Deals a...   \n",
      "2      UPDATE 1-German union Verdi calls for strikes ...   \n",
      "3      Corrections & Amplifications - The success of ...   \n",
      "4      EU mulls wider scope for cybersecurity certifi...   \n",
      "...                                                  ...   \n",
      "95454  Zacks Investment Ideas feature highlights: Alp...   \n",
      "95455  Zacks Investment Ideas feature highlights: Alp...   \n",
      "95456  Zacks Investment Ideas feature highlights: Alp...   \n",
      "95457  Zacks Investment Ideas feature highlights: Alp...   \n",
      "95458     Amazon’s Impact On Consumer Discretionary ETFs   \n",
      "\n",
      "                                             description ticker  \\\n",
      "0      Microsoft chief executive Satya Nadella’s deci...   MSFT   \n",
      "1      The biggest shopping day of the season is upon...   COST   \n",
      "2      German trade union Verdi has called on members...   AMZN   \n",
      "3      The success of blood thinners being developed ...    BMY   \n",
      "4      The European Union is considering broadening t...  GOOGL   \n",
      "...                                                  ...    ...   \n",
      "95454  Alphabet, Amazon and Tesla have been highlight...  GOOGL   \n",
      "95455  Alphabet, Amazon and Tesla have been highlight...   TSLA   \n",
      "95456  Alphabet, Amazon and Tesla have been highlight...   AMZN   \n",
      "95457  Alphabet, Amazon and Tesla have been highlight...   GOOG   \n",
      "95458  The company has been having a rough year, and ...   AMZN   \n",
      "\n",
      "                            company                  sector  \\\n",
      "0             Microsoft Corporation              Technology   \n",
      "1      Costco Wholesale Corporation      Consumer Defensive   \n",
      "2                  Amazon.com, Inc.       Consumer Cyclical   \n",
      "3      Bristol-Myers Squibb Company              Healthcare   \n",
      "4                     Alphabet Inc.  Communication Services   \n",
      "...                             ...                     ...   \n",
      "95454                 Alphabet Inc.  Communication Services   \n",
      "95455                   Tesla, Inc.       Consumer Cyclical   \n",
      "95456              Amazon.com, Inc.       Consumer Cyclical   \n",
      "95457                 Alphabet Inc.  Communication Services   \n",
      "95458              Amazon.com, Inc.       Consumer Cyclical   \n",
      "\n",
      "                             industry  change_pct  \n",
      "0             Software—Infrastructure   -0.500163  \n",
      "1                     Discount Stores    0.592448  \n",
      "2                     Internet Retail   -0.825589  \n",
      "3        Drug Manufacturers - General    0.323559  \n",
      "4      Internet Content & Information   -1.429600  \n",
      "...                               ...         ...  \n",
      "95454  Internet Content & Information  -16.099594  \n",
      "95455              Auto Manufacturers   18.298315  \n",
      "95456                 Internet Retail  -17.533490  \n",
      "95457  Internet Content & Information  -16.361722  \n",
      "95458                 Internet Retail  -17.810797  \n",
      "\n",
      "[95459 rows x 8 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentiment data with historical data\n",
    "def merge_data(historical, news, ticker):\n",
    "    historical = historical[historical['Ticker'] == ticker]\n",
    "    news = news[news['ticker'] == ticker]\n",
    "    merged_data = pd.merge(historical, news, left_on='Date', right_on='datetime', how='left')\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    return merged_data\n",
    "\n",
    "# Example for one ticker (loop through all for full model creation)\n",
    "example_ticker = tickers[0]\n",
    "merged_data = merge_data(historical_df, news_data, example_ticker)\n",
    "\n",
    "# Prepare datasets for LSTM models\n",
    "x_hist, y_hist, scaler = preprocess_data(merged_data, \"Close\")\n",
    "\n",
    "# Add sentiment scores for second model\n",
    "x_with_sentiment = np.hstack([x_hist, merged_data['sentiment_score'].values.reshape(-1, 1)])\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train_hist, x_test_hist, y_train_hist, y_test_hist = train_test_split(x_hist, y_hist, test_size=0.2, random_state=42)\n",
    "x_train_sent, x_test_sent, y_train_sent, y_test_sent = train_test_split(x_with_sentiment, y_hist, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model\n",
    "\n",
    "# Model 1: Historical data only\n",
    "model_hist = build_lstm((x_train_hist.shape[1], 1))\n",
    "model_hist.fit(x_train_hist, y_train_hist, epochs=10, batch_size=32, validation_data=(x_test_hist, y_test_hist))\n",
    "\n",
    "# Model 2: Historical data + Sentiment scores\n",
    "model_sent = build_lstm((x_train_sent.shape[1], 1))\n",
    "model_sent.fit(x_train_sent, y_train_sent, epochs=10, batch_size=32, validation_data=(x_test_sent, y_test_sent))\n",
    "\n",
    "# Save models\n",
    "model_hist.save(\"lstm_hist_model.h5\")\n",
    "model_sent.save(\"lstm_sent_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
